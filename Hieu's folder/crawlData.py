from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.common.keys import Keys
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from bs4 import BeautifulSoup
import pandas as pd
import time
import random
import undetected_chromedriver as uc
from selenium.common.exceptions import TimeoutException


# ------------------------------
# 1. Setup WebDriver
# ------------------------------
def setup_driver():
    options = uc.ChromeOptions()
    options.add_argument('--disable-gpu')
    options.add_argument('--no-sandbox')
    options.add_argument('--disable-dev-shm-usage')
    options.add_argument('user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36')
    options.add_argument('--disable-blink-features=AutomationControlled')
    driver = uc.Chrome(options=options)
    return driver

def get_text_or_empty(element):
    """Tr·∫£ v·ªÅ vƒÉn b·∫£n t·ª´ ph·∫ßn t·ª≠ n·∫øu n√≥ t·ªìn t·∫°i, n·∫øu kh√¥ng tr·∫£ v·ªÅ chu·ªói r·ªóng."""
    return element.text.strip() if element else ''

def get_attr_or_empty(element, attr):
    """L·∫•y gi√° tr·ªã thu·ªôc t√≠nh t·ª´ ph·∫ßn t·ª≠ n·∫øu n√≥ t·ªìn t·∫°i, n·∫øu kh√¥ng tr·∫£ v·ªÅ chu·ªói r·ªóng."""
    return element.get(attr, '').strip() if element else ''
# ------------------------------
# 2. H√†m Crawl D·ªØ Li·ªáu
# ------------------------------
def crawl_topcv(keyword):
    driver = setup_driver()
    jobs_data = []
    
    try:
        driver.get("https://www.topcv.vn")
        print("‚úÖ ƒê√£ truy c·∫≠p trang TopCV th√†nh c√¥ng.")
        time.sleep(random.uniform(3, 5))  # Ch·ªù ng·∫´u nhi√™n
        
        # T√¨m √¥ t√¨m ki·∫øm
        search_box = WebDriverWait(driver, 20).until(
            EC.presence_of_element_located((By.ID, 'keyword'))
        )
        print("‚úÖ ƒê√£ t√¨m th·∫•y √¥ t√¨m ki·∫øm.")
        
        # Nh·∫≠p t·ª´ kh√≥a v√† t√¨m ki·∫øm
        search_box.send_keys(keyword)
        time.sleep(random.uniform(1, 3))
        search_box.send_keys(Keys.RETURN)
        print(f"üîç ƒê√£ nh·∫≠p t·ª´ kh√≥a '{keyword}' v√† b·∫Øt ƒë·∫ßu t√¨m ki·∫øm.")
        time.sleep(random.uniform(3, 5))
        
        # L·∫•y s·ªë l∆∞·ª£ng trang
        soup = BeautifulSoup(driver.page_source, 'html.parser')
        pagination_text = soup.find('span', id='job-listing-paginate-text').text.strip()
        
        # Lo·∫°i b·ªè k√Ω t·ª± kh√¥ng h·ª£p l·ªá (non-breaking space)
        pagination_text = pagination_text.replace('\xa0', ' ')  # Thay th·∫ø k√Ω t·ª± '\xa0' b·∫±ng kho·∫£ng tr·∫Øng th√¥ng th∆∞·ªùng
        
        # Tr√≠ch xu·∫•t s·ªë trang
        num_pages = int(pagination_text.split(' ')[-2])
        print(f"S·ªë l∆∞·ª£ng trang: {num_pages}")

        # L·∫∑p qua c√°c trang
        for page in range(num_pages):
            print(f"üìÑ ƒêang thu th·∫≠p d·ªØ li·ªáu t·ª´ trang {page + 1}...")

            # ƒê·∫£m b·∫£o trang ƒë√£ t·∫£i
            WebDriverWait(driver, 20).until(
                EC.presence_of_element_located((By.CLASS_NAME, 'job-list-search-result'))
            )

            soup = BeautifulSoup(driver.page_source, 'html.parser')
            job_listings = soup.find_all('div', class_='job-list-search-result')  # C·∫≠p nh·∫≠t class name t·∫°i ƒë√¢y

            if not job_listings:
                print("‚ùå Kh√¥ng t√¨m th·∫•y tin tuy·ªÉn d·ª•ng tr√™n trang n√†y.")
                continue
            
            for job in job_listings:
                job_items = job.find_all('div', class_='job-item-search-result')
                for job in job_items:
                    try:
                        job_data = {
                            'Job Title': get_text_or_empty(job.find('h3', class_='title')),
                            'Role': get_text_or_empty(job.find('a', class_='company job-pro')),  # Role t·ª´ c√¥ng ty
                            'Level': get_text_or_empty(job.find('label', class_='title-salary')),  # M·ª©c l∆∞∆°ng
                            'Years of Experience': get_text_or_empty(job.find('label', class_='exp')),  # Kinh nghi·ªám
                            'Company': get_text_or_empty(job.find('span', class_='company-name')),  # T√™n c√¥ng ty
                            'Location': get_text_or_empty(job.find('span', class_='city-text')),  # V·ªã tr√≠
                            'Salary Range': get_text_or_empty(job.find('label', class_='title-salary')),  # M·ª©c l∆∞∆°ng
                            'Required Skills': get_text_or_empty(job.find('div', class_='tag')),  # K·ªπ nƒÉng y√™u c·∫ßu
                            'Job URL': get_attr_or_empty(job.find('a'), 'href'),  # URL c·ªßa c√¥ng vi·ªác
                            'Source Platform': 'TopCV'
                        }
                        jobs_data.append(update_jobs_data_with_details(job_data))
                    except AttributeError as e:
                        print(f"error ", e)
                        continue
            
            print(f"‚úÖ ƒê√£ thu th·∫≠p d·ªØ li·ªáu t·ª´ trang {page + 1}.")
            time.sleep(random.uniform(2, 4))
            
            # Chuy·ªÉn trang
            try:
                # S·ª≠ d·ª•ng c√°ch t√¨m n√∫t "Next" b·∫±ng thu·ªôc t√≠nh 'rel' ho·∫∑c 'aria-label'
                next_button = WebDriverWait(driver, 10).until(
                    EC.element_to_be_clickable((By.CSS_SELECTOR, 'a[rel="next"]'))
                )
                next_button.click()
                print("‚û°Ô∏è ƒêang chuy·ªÉn sang trang ti·∫øp theo.")
                time.sleep(random.uniform(3, 5))
            except Exception as e:
                print(f"üö´ Kh√¥ng t√¨m th·∫•y n√∫t chuy·ªÉn trang. L·ªói: {e}")
                break
                
    except Exception as e:
        print(f"üö® L·ªói trong qu√° tr√¨nh thu th·∫≠p d·ªØ li·ªáu: {e}")
        
    finally:
        try:
            # ƒê·∫£m b·∫£o vi·ªác ƒë√≥ng driver kh√¥ng g√¢y l·ªói
            if driver:
                driver.quit()
                print("üö™ ƒê√£ ƒë√≥ng tr√¨nh duy·ªát.")
        except Exception as e:
            print(f"‚ùå L·ªói khi ƒë√≥ng tr√¨nh duy·ªát: {e}")
    print(f"‚úÖ D·ªØ li·ªáu thu th·∫≠p ƒë∆∞·ª£c: {len(jobs_data)} c√¥ng vi·ªác.")
    return jobs_data

def update_jobs_data_with_details(jobs_data):
    driver = setup_driver()  # Kh·ªüi t·∫°o Selenium WebDriver

    try:
        job_url = jobs_data.get('Job URL')
        if not job_url:
            print("üö´ Kh√¥ng c√≥ URL c√¥ng vi·ªác. B·ªè qua.")
        # Truy c·∫≠p URL c√¥ng vi·ªác
        driver.get(job_url)
        print(f"‚úÖ ƒê√£ truy c·∫≠p th√†nh c√¥ng: {job_url}")
        time.sleep(random.uniform(3, 5))  # Ch·ªù ng·∫´u nhi√™n ƒë·ªÉ tr√°nh b·ªã ch·∫∑n

        # ƒê·∫£m b·∫£o trang ƒë√£ t·∫£i xong ph·∫ßn t·ª≠ c·∫ßn thi·∫øt
        try:
            WebDriverWait(driver, 20).until(
                EC.presence_of_element_located((By.CLASS_NAME, 'box-general-group-info-value'))
            )
        except TimeoutException:
            print("üö® Kh√¥ng t√¨m th·∫•y ph·∫ßn t·ª≠ c·∫ßn thi·∫øt tr√™n trang. B·ªè qua c√¥ng vi·ªác n√†y.")

        # D√πng BeautifulSoup ƒë·ªÉ ph√¢n t√≠ch n·ªôi dung trang
        soup = BeautifulSoup(driver.page_source, 'html.parser')

        # L·∫•y th√¥ng tin c·∫•p b·∫≠c
        cap_bac = soup.find('div', class_='box-general-group-info-value')
        jobs_data['Level'] = cap_bac.text.strip() if cap_bac else 'N/A'

        # L·∫•y k·ªπ nƒÉng c·∫ßn c√≥
        skills_section = soup.find('div', class_='box-category collapsed')
        if skills_section:
            # T√¨m ph·∫ßn t·ª≠ c√≥ n·ªôi dung "K·ªπ nƒÉng c·∫ßn c√≥"
            skills_tags = skills_section.find('div', class_='box-category-tags').find_all('a')
            skills_list = [skill.text.strip() for skill in skills_tags]
            jobs_data['Required Skills'] = ', '.join(skills_list)
        else:
            jobs_data['Required Skills'] = 'N/A'


        print(f"üîÑ ƒê√£ c·∫≠p nh·∫≠t th√¥ng tin chi ti·∫øt cho c√¥ng vi·ªác: {jobs_data['Job Title']}")

    except Exception as e:
        print(f"üö® L·ªói khi x·ª≠ l√Ω c√¥ng vi·ªác '{job.get('Job Title', 'N/A')}': {e}")
    finally:
        try:
            if driver:
                driver.quit()
                print("üö™ ƒê√£ ƒë√≥ng tr√¨nh duy·ªát.")
        except Exception as e:
            print(f"‚ùå L·ªói khi ƒë√≥ng tr√¨nh duy·ªát: {e}")


    print("‚úÖ Ho√†n t·∫•t c·∫≠p nh·∫≠t d·ªØ li·ªáu chi ti·∫øt.")
    print("‚ùå job_data", jobs_data)

    return jobs_data



# ------------------------------
# 3. L∆∞u D·ªØ Li·ªáu Th√†nh CSV
# ------------------------------
def save_to_csv(jobs_data, filename='D://merged_data.csv'):
    import os
    os.makedirs(os.path.dirname(filename), exist_ok=True)
    
    # Ki·ªÉm tra d·ªØ li·ªáu thu th·∫≠p tr∆∞·ªõc khi l∆∞u
    if not jobs_data:
        print("‚ùå Kh√¥ng c√≥ d·ªØ li·ªáu ƒë·ªÉ l∆∞u.")
        return
    
    df = pd.DataFrame(jobs_data)
    
    # L∆∞u d·ªØ li·ªáu v√†o CSV v·ªõi m√£ h√≥a utf-8-sig ƒë·ªÉ gi·ªØ d·∫•u ti·∫øng Vi·ªát
    df.to_csv(filename, index=False, encoding='utf-8-sig')
    print(f"üíæ D·ªØ li·ªáu ƒë√£ ƒë∆∞·ª£c l∆∞u v√†o '{filename}'.")

# ------------------------------
# 4. Ch·∫°y Ch∆∞∆°ng Tr√¨nh Ch√≠nh
# ------------------------------
def main():
    keyword = 'IT'
    print(f"üîë T·ª´ kh√≥a t√¨m ki·∫øm: {keyword}")
    jobs_data = crawl_topcv(keyword)
    save_to_csv(jobs_data)

# ------------------------------
# 5. ƒêi·ªÉm B·∫Øt ƒê·∫ßu
# ------------------------------
if __name__ == "__main__":
    main()
